{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ac50c0d-776c-461a-93a7-2cbe83959bdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Set Shape: (1618172, 15), Labels: (1618172,)\n",
      "Test Set Shape: (404543, 15), Labels: (404543,)\n",
      "\n",
      "Encoded Classes: ['Benign' 'Brute Force -Web' 'Brute Force -XSS' 'DDoS attacks-LOIC-HTTP'\n",
      " 'DoS attacks-GoldenEye' 'DoS attacks-Slowloris' 'Infilteration']\n",
      "Training XGBoost model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\S569652\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:158: UserWarning: [20:52:44] WARNING: C:\\buildkite-agent\\builds\\buildkite-windows-cpu-autoscaling-group-i-08cbc0333d8d4aae1-1\\xgboost\\xgboost-ci-windows\\src\\learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 77\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining XGBoost model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 77\u001b[0m xgb_model\u001b[38;5;241m.\u001b[39mfit(X_train, y_train_encoded)\n\u001b[0;32m     79\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;66;03m# Step 4: Model Evaluation\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;66;03m# ===============================\u001b[39;00m\n\u001b[0;32m     82\u001b[0m \n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Predict on test set\u001b[39;00m\n\u001b[0;32m     84\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m xgb_model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\sklearn.py:1599\u001b[0m, in \u001b[0;36mXGBClassifier.fit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights)\u001b[0m\n\u001b[0;32m   1579\u001b[0m model, metric, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_configure_fit(xgb_model, params)\n\u001b[0;32m   1580\u001b[0m train_dmatrix, evals \u001b[38;5;241m=\u001b[39m _wrap_evaluation_matrices(\n\u001b[0;32m   1581\u001b[0m     missing\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmissing,\n\u001b[0;32m   1582\u001b[0m     X\u001b[38;5;241m=\u001b[39mX,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1596\u001b[0m     feature_types\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_types,\n\u001b[0;32m   1597\u001b[0m )\n\u001b[1;32m-> 1599\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_Booster \u001b[38;5;241m=\u001b[39m train(\n\u001b[0;32m   1600\u001b[0m     params,\n\u001b[0;32m   1601\u001b[0m     train_dmatrix,\n\u001b[0;32m   1602\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_boosting_rounds(),\n\u001b[0;32m   1603\u001b[0m     evals\u001b[38;5;241m=\u001b[39mevals,\n\u001b[0;32m   1604\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds,\n\u001b[0;32m   1605\u001b[0m     evals_result\u001b[38;5;241m=\u001b[39mevals_result,\n\u001b[0;32m   1606\u001b[0m     obj\u001b[38;5;241m=\u001b[39mobj,\n\u001b[0;32m   1607\u001b[0m     custom_metric\u001b[38;5;241m=\u001b[39mmetric,\n\u001b[0;32m   1608\u001b[0m     verbose_eval\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[0;32m   1609\u001b[0m     xgb_model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m   1610\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks,\n\u001b[0;32m   1611\u001b[0m )\n\u001b[0;32m   1613\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective):\n\u001b[0;32m   1614\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobjective \u001b[38;5;241m=\u001b[39m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjective\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:726\u001b[0m, in \u001b[0;36mrequire_keyword_args.<locals>.throw_if.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    724\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(sig\u001b[38;5;241m.\u001b[39mparameters, args):\n\u001b[0;32m    725\u001b[0m     kwargs[k] \u001b[38;5;241m=\u001b[39m arg\n\u001b[1;32m--> 726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:181\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, custom_metric)\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mbefore_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    180\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m--> 181\u001b[0m bst\u001b[38;5;241m.\u001b[39mupdate(dtrain, iteration\u001b[38;5;241m=\u001b[39mi, fobj\u001b[38;5;241m=\u001b[39mobj)\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cb_container\u001b[38;5;241m.\u001b[39mafter_iteration(bst, i, dtrain, evals):\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\Lib\\site-packages\\xgboost\\core.py:2101\u001b[0m, in \u001b[0;36mBooster.update\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   2097\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_assign_dmatrix_features(dtrain)\n\u001b[0;32m   2099\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   2100\u001b[0m     _check_call(\n\u001b[1;32m-> 2101\u001b[0m         _LIB\u001b[38;5;241m.\u001b[39mXGBoosterUpdateOneIter(\n\u001b[0;32m   2102\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandle, ctypes\u001b[38;5;241m.\u001b[39mc_int(iteration), dtrain\u001b[38;5;241m.\u001b[39mhandle\n\u001b[0;32m   2103\u001b[0m         )\n\u001b[0;32m   2104\u001b[0m     )\n\u001b[0;32m   2105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2106\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict(dtrain, output_margin\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Notebook: XGBoost Model Training for INADS\n",
    "------------------------------------------\n",
    "Objective:\n",
    "- Train an XGBoost model for anomaly detection in network traffic.\n",
    "- Evaluate model performance using accuracy, confusion matrix, and ROC-AUC.\n",
    "- Compare with Random Forest baseline results.\n",
    "\n",
    "Dataset:\n",
    "- Preprocessed train and test sets (train_set_fixed.csv, test_set_fixed.csv).\n",
    "- Label encoding applied for multi-class classification.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# ===============================\n",
    "# Step 1: Load the Train and Test Sets\n",
    "# ===============================\n",
    "\n",
    "# Define dataset paths\n",
    "train_path = r\"C:\\Users\\S569652\\Documents\\INADS\\data\\train_set_fixed.csv\"\n",
    "test_path = r\"C:\\Users\\S569652\\Documents\\INADS\\data\\test_set_fixed.csv\"\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(train_path)\n",
    "test_df = pd.read_csv(test_path)\n",
    "\n",
    "# Separate features and labels\n",
    "X_train = train_df.drop(columns=[\"Label\"])  # Features\n",
    "y_train = train_df[\"Label\"]  # Labels\n",
    "\n",
    "X_test = test_df.drop(columns=[\"Label\"])  # Features\n",
    "y_test = test_df[\"Label\"]  # Labels\n",
    "\n",
    "# Check dataset shapes\n",
    "print(f\"Train Set Shape: {X_train.shape}, Labels: {y_train.shape}\")\n",
    "print(f\"Test Set Shape: {X_test.shape}, Labels: {y_test.shape}\")\n",
    "\n",
    "# ===============================\n",
    "# Step 2: Encode Labels\n",
    "# ===============================\n",
    "\n",
    "# Convert categorical labels to numerical format\n",
    "label_encoder = LabelEncoder()\n",
    "y_train_encoded = label_encoder.fit_transform(y_train)\n",
    "y_test_encoded = label_encoder.transform(y_test)\n",
    "\n",
    "# Display encoded class mapping\n",
    "print(\"\\nEncoded Classes:\", label_encoder.classes_)\n",
    "\n",
    "# ===============================\n",
    "# Step 3: Train the XGBoost Model\n",
    "# ===============================\n",
    "\n",
    "# Define the XGBoost classifier\n",
    "xgb_model = XGBClassifier(\n",
    "    n_estimators=200,       # Number of trees\n",
    "    max_depth=6,            # Depth of each tree\n",
    "    learning_rate=0.1,      # Step size shrinkage for better optimization\n",
    "    subsample=0.8,          # Fraction of training samples used per tree\n",
    "    colsample_bytree=0.8,   # Fraction of features used per tree\n",
    "    random_state=42,\n",
    "    eval_metric=\"mlogloss\"\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "print(\"Training XGBoost model...\")\n",
    "xgb_model.fit(X_train, y_train_encoded)\n",
    "\n",
    "# ===============================\n",
    "# Step 4: Model Evaluation\n",
    "# ===============================\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "train_acc = accuracy_score(y_train_encoded, xgb_model.predict(X_train))\n",
    "test_acc = accuracy_score(y_test_encoded, y_pred)\n",
    "\n",
    "print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\nClassification Report (Test Set):\")\n",
    "print(classification_report(y_test_encoded, y_pred, target_names=label_encoder.classes_))\n",
    "\n",
    "# ===============================\n",
    "# Step 5: Confusion Matrix\n",
    "# ===============================\n",
    "\n",
    "# Generate Confusion Matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(\n",
    "    confusion_matrix(y_test_encoded, y_pred),\n",
    "    annot=True,\n",
    "    fmt=\"d\",\n",
    "    cmap=\"Blues\",\n",
    "    xticklabels=label_encoder.classes_,\n",
    "    yticklabels=label_encoder.classes_\n",
    ")\n",
    "plt.title(\"Confusion Matrix - XGBoost Model\", fontsize=14)\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# Step 6: Feature Importance\n",
    "# ===============================\n",
    "\n",
    "# Extract feature importance scores\n",
    "feature_importance = xgb_model.feature_importances_\n",
    "\n",
    "# Create DataFrame for visualization\n",
    "feature_df = pd.DataFrame({\"Feature\": X_train.columns, \"Importance\": feature_importance})\n",
    "feature_df = feature_df.sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Plot Feature Importance\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.barh(feature_df[\"Feature\"], feature_df[\"Importance\"], color=\"blue\")\n",
    "plt.xlabel(\"Feature Importance Score\")\n",
    "plt.ylabel(\"Feature Name\")\n",
    "plt.title(\"Feature Importance - XGBoost Model\")\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# Step 7: ROC-AUC Curve\n",
    "# ===============================\n",
    "\n",
    "# Compute ROC-AUC score for each class\n",
    "y_proba = xgb_model.predict_proba(X_test)\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for i, class_label in enumerate(label_encoder.classes_):\n",
    "    fpr, tpr, _ = roc_curve(y_test_encoded == i, y_proba[:, i])\n",
    "    auc = roc_auc_score(y_test_encoded == i, y_proba[:, i])\n",
    "    plt.plot(fpr, tpr, label=f\"{class_label} (AUC: {auc:.2f})\")\n",
    "\n",
    "# Plot diagonal baseline\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC-AUC Curve - XGBoost\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# ===============================\n",
    "# Final Results\n",
    "# ===============================\n",
    "\n",
    "print(\"\\nFinal Evaluation Summary:\")\n",
    "print(f\"Train Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "print(\"Feature importance and confusion matrix plotted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12021acd-df2d-443a-b668-9f3307dea165",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
